{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math for mathematical functions\n",
    "import math\n",
    "\n",
    "# numpy for linear algebra and sampling\n",
    "import numpy as np\n",
    "\n",
    "# pandas for data processing\n",
    "import pandas as pd \n",
    "\n",
    "# keras for neural network\n",
    "from keras.models import load_model\n",
    "from keras import backend\n",
    "\n",
    "# Tensorflow for neural nerwork model gradients\n",
    "import tensorflow as tf\n",
    "\n",
    "# matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# sklearn for k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# sklearn for k-nearest neighbors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "# sklearn for balltree\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# for k-d tree\n",
    "from scipy import spatial\n",
    "\n",
    "# scipy for optimization library\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# itertools for generating scenarios\n",
    "import itertools\n",
    "\n",
    "# time for timing methods\n",
    "import time\n",
    "\n",
    "# Import population data\n",
    "df_florida = pd.read_csv('CleanData/Florida-US-CENSUS-and-Voter-DATA.csv')\n",
    "# Drop fist column (as it is 'Unnamed' column not containing real data)\n",
    "df_florida.drop(df_florida.columns[0],axis=1,inplace=True)\n",
    "# Save header names for data frame\n",
    "df_florida_header_list = [i for i in list(df_florida.columns.values) if i not in ['County']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Functions associated with Redistricting ------------------------ #\n",
    "\n",
    "# Function for generating possibly non-contiguous district\n",
    "def construct_district(n_dist, n_neigh, pts, P, R, p_norm):\n",
    "    # Create copy of P so P does not get destroyed\n",
    "    Q = P[:]\n",
    "    \n",
    "    # Create copy of R so R does not get destroyed\n",
    "    S = R[:]\n",
    "    \n",
    "    # Initialize n_race\n",
    "    n_race = len(S)\n",
    "    \n",
    "    # Create array of means of population race distributions\n",
    "    mu = []\n",
    "    for i in range(n_race):\n",
    "        mu = np.append(mu, np.mean(S[i]))    \n",
    "        \n",
    "    # Initialize mean of race distribution for pts\n",
    "    R_means = np.zeros((n_dist, n_race))\n",
    "    \n",
    "    # Initialize counter for each district\n",
    "    n_pts = np.ones(n_dist, dtype = int)\n",
    "    \n",
    "    # Initialize list of remaining indices\n",
    "    rem_indices = [i for i in range(len(P))]\n",
    "    \n",
    "    # Initialize x_dist, y_dist, and ind_dist used to store x and y coords of pts in each district\n",
    "    # and the indices of P belonging to each district\n",
    "    x_dist = [[] for i in range(n_dist)]\n",
    "    y_dist = [[] for i in range(n_dist)]\n",
    "    ind_dist = [[] for i in range(n_dist)]\n",
    "    \n",
    "    while len(Q) > n_neigh:\n",
    "        for i in range(n_dist):\n",
    "            # Create KDTree\n",
    "            tree = BallTree(Q) #, leaf_size = 100)\n",
    "            \n",
    "            # Set number of neighbors to use in update of indices and distances \n",
    "            # (need to make sure we don't exceed avaiable number of points left)\n",
    "            neighbors = min(n_neigh,len(Q)-1) \n",
    "            # Determine distances and indices from nearest neighbor search\n",
    "            dist, indices = tree.query(pts[i], k=neighbors)\n",
    "            \n",
    "            # Initialize index of point\n",
    "            indices = indices.flatten()\n",
    "            ind = indices[0]\n",
    "            \n",
    "            # Initialize min_dist\n",
    "            min_dist = 100*n_race\n",
    "            # Initialize temp mean array\n",
    "            temp_mean = np.zeros(n_race)\n",
    "\n",
    "            # Determine index which minimizes distances from mean of race distributions\n",
    "            for j in range(neighbors):\n",
    "                temp_ind = indices[j]\n",
    "                # Update race means for test point\n",
    "                for k in range(n_race):\n",
    "                    temp_mean[k] = ((n_pts[i]-1)*R_means[i][k] + S[k][temp_ind])/(n_pts[i])\n",
    "                # Compute distance to mean for each race and sum\n",
    "                temp_dist = np.mean(temp_mean - mu)\n",
    "                # Check if distance from mean is smaller than current smallest value\n",
    "                if temp_dist < min_dist:\n",
    "                    # Update index\n",
    "                    ind = temp_ind\n",
    "                    # Update min_dist\n",
    "                    min_dist = temp_dist  \n",
    "                    # Update mean\n",
    "                    new_means = np.copy(temp_mean)\n",
    "                    \n",
    "            # Update R_means\n",
    "            R_means[i] = np.copy(new_means)\n",
    "            \n",
    "            # Append x and y values to district lists\n",
    "            x_dist[i] = np.append(x_dist[i], Q[ind][0])\n",
    "            y_dist[i] = np.append(y_dist[i], Q[ind][1])\n",
    "            \n",
    "            # Appending indices to index list\n",
    "            ind_dist[i] = np.append(ind_dist[i], rem_indices[ind])\n",
    "\n",
    "            # Update counter\n",
    "            n_pts[i] = n_pts[i] + 1\n",
    "            \n",
    "            # Update pts[i]\n",
    "            #for j in range(len(pts[i])):\n",
    "            #    pts[i][j] = np.sum(pts[i][j])/n_pts[i]\n",
    "            \n",
    "            # Update Q by removing point added to district\n",
    "            Q = np.delete(Q, ind, 0)\n",
    "            \n",
    "            # Update S by removing point added to district\n",
    "            for k in range(n_race):\n",
    "                S[k] = np.delete(S[k], ind, 0)\n",
    "            \n",
    "            # Update remaining indices by removing used index\n",
    "            rem_indices = np.delete(rem_indices, ind)\n",
    "            \n",
    "    # Return x_dist and y_dist and exit function\n",
    "    return x_dist, y_dist, ind_dist, R_means\n",
    "\n",
    "# Function for creating array with number of points per county in each district\n",
    "def county_per_district(n_dist, n_counties, county_arr, ind_arr):\n",
    "    # Create array containing number of points from each county in each district\n",
    "    Y_counties = np.zeros((n_dist, n_counties), dtype = int)\n",
    "    \n",
    "    j = len(ind_arr)\n",
    "    k = len(county_arr)\n",
    "    print(\"j,k: %lg, %lg\" %(j,k))\n",
    "    \n",
    "    # Fill array with appropriate values\n",
    "    for i in range(len(ind_arr)):\n",
    "        Y_counties[int(ind_arr[i])][int(county_arr[i])] = Y_counties[int(ind_arr[i])][int(county_arr[i])] + 1\n",
    "        \n",
    "    # Return Y_counties\n",
    "    return Y_counties\n",
    "\n",
    "# Function for creating district data based on US CENSUS and Voter Registration Data for counties\n",
    "def create_district_data(X_pops, X_medians, Y_counties, n_dist, n_counties, pop_scale):        \n",
    "    # Initialize array to store features for districts\n",
    "    Y_pops = np.zeros((n_dist, np.shape(X_pops)[1]))\n",
    "    Y_medians = np.zeros((n_dist, np.shape(X_medians)[1]))\n",
    "    \n",
    "    # Update medians by averaging medians\n",
    "    Y_medians = np.matmul(Y_counties, X_medians)\n",
    "    count_per_dist = np.matmul(Y_counties, np.ones(n_counties))\n",
    "    \n",
    "    # Fill population arrays\n",
    "    for j in range(n_dist):\n",
    "        Y_medians[j] = Y_medians[j] / (count_per_dist[j])\n",
    "        #temp = np.zeros(np.shape(X_medians)[1])\n",
    "        # Update populations and temp \n",
    "        for i in range(n_counties):\n",
    "            # Update populations\n",
    "            Y_pops[j] = Y_pops[j] + (Y_counties[j][i] / pop_scale) * X_pops[i]\n",
    "            # Update temp for averaging medians\n",
    "            #temp = temp + Y_counties[j][i] * X_medians[i]\n",
    "            \n",
    "    # Return arrays containing population of each district and medians of each district\n",
    "    return Y_pops, Y_medians \n",
    "\n",
    "\n",
    "\n",
    "# ------------------- Functions for Classifying and Plotting Districts ------------------- #\n",
    "# ------------ Initial classification of districts ------- #\n",
    "def initial_district_classification(P, indk, district_number):\n",
    "    # ------------------- Train nearest neighbors model for classification to enforce contiguity districts --------------\n",
    "    # Our features will be the set of ordered pairs P\n",
    "    X_train = P\n",
    "    # The target data will be the classes determined by partitioning of the points using the construct_districts function\n",
    "    Y_train = np.zeros(len(P), dtype = int)\n",
    "    for i in range(num_districts):\n",
    "        for j in range(len(indk[i])):\n",
    "            t = int(indk[i][j])\n",
    "            Y_train[t] = i    \n",
    "\n",
    "    # --------------- Initial Classification and View of Districts ------------------ #\n",
    "    # Set the step size in the mesh (large since data points are fairly spread out)\n",
    "    h = 2500\n",
    "\n",
    "    # Set number of neighbors to use (larger number seems to force regions to be contiguous)\n",
    "    n_neighbors = 200\n",
    "\n",
    "    # Set norm to use when computing distance (using euclidean norm, seems to work well enough)\n",
    "    nrm = 2\n",
    "\n",
    "    # Create color maps for the classes\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    for weights in ['distance', 'uniform']:\n",
    "        # we create an instance of Neighbours Classifier and fit the data.\n",
    "        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights, p=nrm)\n",
    "        clf.fit(X_train, Y_train)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "        plt.fill([501000,600000,600000,501000], [201000,201000,700000,700000], 'w', edgecolor='none')\n",
    "\n",
    "        # Plot also the training points\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "        plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (n_neighbors, weights))\n",
    "\n",
    "        image_name = 'Finalized-Districts/Initial-Images/Districts-' + str(district_number) + '-' + weights + '.png'\n",
    "        plt.savefig(image_name)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Return nearest neighbors model which used uniform distances\n",
    "    return clf\n",
    "\n",
    "# ------------ Final classification of districts ------- #\n",
    "def final_district_classification(P, clf, district_number):\n",
    "    # ------------------ Use Nearest Neighbors Classifier to Predict district for each population point ---------------- #\n",
    "    # Our features will be the set of ordered pairs P\n",
    "    X_train = P   \n",
    "    # Use the model to finalize districts        \n",
    "    Y_dist = clf.predict(X_train)\n",
    "    \n",
    "    # Set number of neighbors to use (larger number seems to force regions to be contiguous)\n",
    "    n_neighbors = 200\n",
    "\n",
    "    # Initialize Districts and Races arrays for storing classified points\n",
    "    Districts = [[] for i in range(num_districts)]\n",
    "    Races = [[] for i in range(num_districts)]\n",
    "    for i in range(num_districts):\n",
    "        Races[i] = [[] for j in range(num_races)]\n",
    "\n",
    "    # Set points and race percentages in each district\n",
    "    for i in range(len(Y_dist)):\n",
    "        for j in range(num_districts):\n",
    "            if j == Y_dist[i]:\n",
    "                Districts[j] = np.append(Districts[j], X_train[i])\n",
    "                for k in range(num_races):\n",
    "                    Races[j][k] = np.append(Races[j][k], R[k][i])\n",
    "                break;\n",
    "\n",
    "    # Set filename for writing data\n",
    "    file_name = 'Finalized-Districts/Race-Distributions/Districts-' + str(district_number) + '-race-distribution.txt'\n",
    "    file = open(file_name, 'w')\n",
    "                \n",
    "    # Print statistics for state population\n",
    "    print(\"-------- Percentage of each Race across State --------\")\n",
    "    file.write(\"-------- Percentage of each Race across State --------\\n\")\n",
    "    for i in range(num_races):\n",
    "        print(\" Percentage of Race %i:           %lg\" %(i, mu_R[i]))\n",
    "        file.write(\" Percentage of Race %i:           %lg\\n\" %(i, mu_R[i]))\n",
    "        file.write\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Determine race means and population distributions of new districts and print\n",
    "    for i in range(num_districts):\n",
    "        print(\"---------------- District %i ----------------\" %(i))\n",
    "        file.write(\"---------------- District %i ----------------\\n\" %(i))\n",
    "        print(\" Population:                     %lg\" %(len(Districts[i])))\n",
    "        file.write(\" Population:                     %lg\\n\" %(len(Districts[i])))\n",
    "        for j in range(num_races):\n",
    "            temp = np.mean(Races[i][j])\n",
    "            print(\" Percentage of Race %i:           %lg\" %(j, temp))\n",
    "            file.write(\" Percentage of Race %i:           %lg\\n\" %(j, temp))\n",
    "            print(\" Deviation from Mean for Race %i: %lg\" %(j, abs(temp - mu_R[j])))\n",
    "            file.write(\" Deviation from Mean for Race %i: %lg\\n\" %(j, abs(temp - mu_R[j])))\n",
    "            \n",
    "    # Close file\n",
    "    file.close()\n",
    "\n",
    "    # -------------------- Final Plot of Districts and Population Denisity Points ---------------------- #\n",
    "    # Set the step size in the mesh\n",
    "    h = 2500\n",
    "\n",
    "    # Create color maps for the classes\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    for i in range(2):\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "        plt.fill([501000,600000,600000,501000], [201000,201000,700000,700000], 'w', edgecolor='none')\n",
    "\n",
    "        # Plot also the training points\n",
    "        if i == 0:\n",
    "            plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_dist, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "            plt.xlim(xx.min(), xx.max())\n",
    "            plt.ylim(yy.min(), yy.max())\n",
    "            plt.title(\"3-Class classification (k = %i, weights = 'uniform')\" % (n_neighbors))\n",
    "            image_name = 'Finalized-Districts/Final-Images/District-' + str(district_number) + '-uniform.png'\n",
    "        else:\n",
    "            #ax2 = plt.add_subplot(111, aspect='equal')\n",
    "            #for p in [\n",
    "            #    patches.Rectangle((0, 0), 20000, 20000, fill=False),\n",
    "            #    patches.Rectangle((20000, 0), 20000, 20000, fill=False),\n",
    "            #]:\n",
    "            #    plt.add_patch(p)\n",
    "            image_name = 'Finalized-Districts/Final-Images/Districts-' + str(district_number) + '-no-pts.png'\n",
    "        \n",
    "        # Save image\n",
    "        plt.savefig(image_name)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return Y_dist\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- Function for generating district data from county data for objective function ---------------------\n",
    "def county_to_district(df_florida, df_florida_header_list, num_districts, num_counties, county_array, Y_dist, district_number):\n",
    "    # Generate number of counties per district\n",
    "    X_counties = county_per_district(num_districts, num_counties, county_array, Y_dist)\n",
    "    #print(X_counties)\n",
    "\n",
    "    # Create matrix of values from data frame corresponding to features encoding population values\n",
    "    X_pops = df_florida.drop(['County', 'Total; Estimate; Median Household income (dollars)', \n",
    "                              'Native; Estimate; Median Household income (dollars)', \n",
    "                              'Foreign born; Estimate; Median Household income (dollars)', \n",
    "                              'Foreign born; Naturalized citizen; Estimate; Median Household income (dollars)', \n",
    "                              'Foreign born; Not a U.S. citizen; Estimate; Median Household income (dollars)'], axis=1).values\n",
    "\n",
    "    # Create matrix of values from data frame corresponding to features encoing median values\n",
    "    X_medians = df_florida[['Total; Estimate; Median Household income (dollars)', \n",
    "                            'Native; Estimate; Median Household income (dollars)', \n",
    "                            'Foreign born; Estimate; Median Household income (dollars)', \n",
    "                            'Foreign born; Naturalized citizen; Estimate; Median Household income (dollars)', \n",
    "                            'Foreign born; Not a U.S. citizen; Estimate; Median Household income (dollars)']].values\n",
    "\n",
    "    # Generate population and median district data\n",
    "    District_pops, District_medians = create_district_data(X_pops, X_medians, X_counties, num_districts, num_counties, pop_scale)\n",
    "\n",
    "    # ---------------- Convert Population values to percentages (configure inputs for neural network) ---------------- #\n",
    "    # Concatenate population and median matrices (containing district information)\n",
    "    X_complete = np.concatenate((District_pops, District_medians), axis = 1)\n",
    "\n",
    "    # Create data frame from district data\n",
    "    df_florida_p = pd.DataFrame(data=np.int_(X_complete), index=range(X_complete.shape[0]), columns=df_florida_header_list)\n",
    "\n",
    "    # Update values in data frame to reflect percentages so they can be used in neural network\n",
    "    # List of headers to exclude as they do not correspond to percentage values\n",
    "    drop_list = ['Total; Estimate; Total population', 'Native; Estimate; Total population', \n",
    "                 'Foreign born; Estimate; Total population', \n",
    "                 'Foreign born; Naturalized citizen; Estimate; Total population', \n",
    "                 'Foreign born; Not a U.S. citizen; Estimate; Total population',\n",
    "                 'Total; Estimate; Median Household income (dollars)', \n",
    "                 'Native; Estimate; Median Household income (dollars)', \n",
    "                 'Foreign born; Estimate; Median Household income (dollars)', \n",
    "                 'Foreign born; Naturalized citizen; Estimate; Median Household income (dollars)', \n",
    "                 'Foreign born; Not a U.S. citizen; Estimate; Median Household income (dollars)']\n",
    "\n",
    "    # List of headers of df_fl with values representing percentages\n",
    "    percentage_list = [i for i in df_florida_header_list if i not in drop_list]\n",
    "\n",
    "    # Create list of header types used in converting raw population values to percentages\n",
    "    header_types = ['Total; Estimate;', 'Native; Estimate;', 'Foreign born; Estimate;', \n",
    "                    'Foreign born; Naturalized citizen; Estimate;', 'Foreign born; Not a U.S. citizen; Estimate;']\n",
    "\n",
    "    # Convert each value from raw value to decimal (representing percentage)\n",
    "    for mystr in percentage_list:\n",
    "        for header in header_types:\n",
    "            if header in mystr:\n",
    "                # Set 'totalstr' to name of header for total population\n",
    "                totalstr = header + ' Total population'\n",
    "                # Convert percentage to value\n",
    "                df_florida_p[mystr] = df_florida_p[mystr] / df_florida_p[totalstr]\n",
    "                break;\n",
    "\n",
    "    # ----------------------- Convert raw population values to percentages --------------------- #\n",
    "    pop_list = ['Active Democrat Voters', 'Active Republican Voters', 'Active Voters with No Party',\n",
    "                'Native; Estimate; Total population', \n",
    "                'Foreign born; Estimate; Total population', \n",
    "                'Foreign born; Naturalized citizen; Estimate; Total population', \n",
    "                'Foreign born; Not a U.S. citizen; Estimate; Total population']\n",
    "\n",
    "    # Initialize string for data frame column containing total population values per district\n",
    "    totalpopstr = 'Total; Estimate; Total population'\n",
    "\n",
    "    for mystr in pop_list:\n",
    "        # Convert population to percentage of population in the district\n",
    "        df_florida_p[mystr] = df_florida_p[mystr] / df_florida_p[totalpopstr]\n",
    "\n",
    "    # Drop total population percentage as it is not needed anymore\n",
    "    df_florida_p.drop([totalpopstr], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    # ------------------ Convert median income values to percentages of $100000 ---------------- #\n",
    "    income_list = ['Total; Estimate; Median Household income (dollars)', \n",
    "                   'Native; Estimate; Median Household income (dollars)', \n",
    "                   'Foreign born; Estimate; Median Household income (dollars)', \n",
    "                   'Foreign born; Naturalized citizen; Estimate; Median Household income (dollars)', \n",
    "                   'Foreign born; Not a U.S. citizen; Estimate; Median Household income (dollars)']\n",
    "\n",
    "    for mystr in income_list:\n",
    "        # Convert population to percentage of population in the district\n",
    "        df_florida_p[mystr] = df_florida_p[mystr] / 100000\n",
    "\n",
    "    # Create matrix of district values to be used in objective function\n",
    "    X_district = df_florida_p.values\n",
    "\n",
    "    # Preview values\n",
    "    df_florida_p.head()\n",
    "    \n",
    "    # Remaining values required for objective function\n",
    "    rem_vars = ['Republican Candidate Spending', 'Democrat Candidate Spending', \n",
    "                      'Party in Office', 'Incumbent Republican Candidate Running', \n",
    "                      'Incumbent Democrat Candidate Running', 'Candidate Party']\n",
    "\n",
    "    # Number of scenarios to consider\n",
    "    n_rows = 6\n",
    "\n",
    "    # Initialize data frame with these variables\n",
    "    df_variables = pd.DataFrame(data=np.zeros((n_rows,len(rem_vars))), index=range(n_rows), columns=rem_vars)\n",
    "\n",
    "    # Add column for scenario and district\n",
    "    df_variables['District'] = [1, 1, 2, 2, 3, 3]\n",
    "    df_variables['Scenario in District'] = [1, 2, 1, 2, 1, 2]\n",
    "    df_variables['Scenario Probability'] = [0.7, 0.3, 0.4, 0.6, 0.8, 0.2]\n",
    "    df_variables['Variable in District'] = ['Democrat Candidate Spending', 'Democrat Candidate Spending',\n",
    "                                            'Republican Candidate Spending', 'Republican Candidate Spending',\n",
    "                                            'Incumbent Candidate', 'Incumbent Candidate']\n",
    "\n",
    "    # Initialize values in each column\n",
    "    df_variables['Republican Candidate Spending'] = np.array([1000000, 2000000, 1000000, 3000000, 2000000, 3000000]) / 4000000\n",
    "    df_variables['Democrat Candidate Spending'] = np.array([1500000, 4000000, 2000000, 4000000, 3000000, 2000000]) / 4000000\n",
    "    df_variables['Party in Office'] = [0, 0, 1, 1, 0, 1]\n",
    "    df_variables['Incumbent Republican Candidate Running'] = [1, 1, 0, 0, 1, 0]\n",
    "    df_variables['Incumbent Democrat Candidate Running'] = [0, 0, 1, 1, 0, 1]\n",
    "    df_variables['Candidate Party'] = np.ones(n_rows, dtype = int)\n",
    "\n",
    "    # Generate all possible scenarios as binary lists\n",
    "    scenario_list = list(itertools.product(*[(0, 1)] * 3))\n",
    "    n_scenarios = len(scenario_list)\n",
    "\n",
    "    # Initialize array to hold scenario values\n",
    "    X_scenarios = np.empty((n_scenarios, num_districts, len(rem_vars) + X_district.shape[1]))\n",
    "\n",
    "    # Initialize array to hold probability that each scenario occurs\n",
    "    p_scenarios = np.empty(n_scenarios)\n",
    "\n",
    "    # Fill array with values for all possible scenarios\n",
    "    for i in range(n_scenarios):\n",
    "        # Rows from data frame required for current scenario\n",
    "        s_rows = [0,2,4] + np.asarray(scenario_list[i])\n",
    "        # Probability of scenario i occuring\n",
    "        p_scenarios[i] = np.prod(df_variables[['Scenario Probability']].iloc[s_rows].values)\n",
    "        # Temp is array of variables from districts for current scenario\n",
    "        temp = df_variables.drop(['District', 'Scenario in District', 'Variable in District', 'Scenario Probability'], axis=1).iloc[s_rows].values\n",
    "        # Prepend district data with variable data for X_scenarios\n",
    "        X_scenarios[i] = np.concatenate((temp, X_district), axis = 1)\n",
    "\n",
    "    #print(p_scenarios)\n",
    "    #print(X_scenarios)\n",
    "\n",
    "    # Initialize names of binary files to save\n",
    "    Xname = 'Numpy-Array-Files/Input-scenarios-Districts-' + str(district_number) + '.npy'\n",
    "    pname = 'Numpy-Array-Files/Probability-scenarios-Districts-' + str(district_number) + '.npy'\n",
    "    \n",
    "    # Save both arrays to binary files\n",
    "    np.save(Xname, X_scenarios)\n",
    "    np.save(pname, p_scenarios)\n",
    "    \n",
    "    # Return number of scenarios and upper bounds for democrat spending\n",
    "    return n_scenarios, X_scenarios[:,:,1].flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- Functions associated with minimizing Campign Expendetures ----------------------------- #\n",
    "# ------------------------------- Objective function --------------------------------- #\n",
    "def f (party_spending):\n",
    "    # ----- Initialize variables to be used in evaluation ----- #\n",
    "    # Set keras model filename location\n",
    "    #keras_model_filename = 'Keras-Models/bin_model.h5'   # binary classification model\n",
    "    #keras_model_filename = 'Keras-Models/dem_model_short_new.h5'\n",
    "    keras_model_filename = 'Keras-Models/nnet_model.h5'\n",
    "    # Set X_scens to contain the input data for all problem scenarios\n",
    "    X_scens = np.load('Numpy-Array-Files/Input_scenarios.npy')\n",
    "    # Set p_scens to contain the probability data for all problem scenarios\n",
    "    P_scens = np.load('Numpy-Array-Files/Probability_scenarios.npy')\n",
    "    # Set n_scens to be the number of scenarios\n",
    "    n_scens = len(P_scens)\n",
    "    # Set n_districts to be the number of districts\n",
    "    n_districts = X_scens.shape[1]\n",
    "    # Array to store values from evaluating keras_model\n",
    "    district_outputs = np.array((n_scens, n_districts))\n",
    "    \n",
    "    # ----- Import new party spending values into scenario data ----- #\n",
    "    # Update X_scens to reflect any changes in party spending (in this case changing amount of democrat spending)\n",
    "    for i in range(n_scens):\n",
    "        for j in range(n_districts):\n",
    "            # Assign (n_districts*i + j)th element of party_spending to X_scens[i,j,1] ('1' is 'Democrat Spending' index)\n",
    "            X_scens[i,j,1] = party_spending[n_districts * i + j]\n",
    "    \n",
    "    # ----- Set up keras model to evaluate function ----- #\n",
    "    model = load_model(keras_model_filename)\n",
    "            \n",
    "    # ----- Compute objective value as expected value of probability democrat candidate will win ----- #\n",
    "    # Initialize value to 0\n",
    "    value = 0\n",
    "    for i in range(n_scens):\n",
    "        # Initialize temp to 0\n",
    "        temp = 0\n",
    "        # Evaluate keras model for current scenario data\n",
    "        arr_temp = model.predict(X_scens[i])\n",
    "        #print(\"arr_temp = \", arr_temp)\n",
    "\n",
    "        # Sum all district values up in ith scenario\n",
    "        for j in range(n_districts):\n",
    "            # Add district_output value for jth district to temp\n",
    "            #temp = temp + arr_temp[j][1]      # this line is using binary classification model\n",
    "            temp = temp + arr_temp[j]        # this line if predicting percentage of votes\n",
    "\n",
    "        # Multiply temp by probability scenario i occurs then add to value\n",
    "        value = value + P_scens[i] * temp\n",
    "\n",
    "    return -value\n",
    "    \n",
    "    \n",
    "    \n",
    "# ------------------- Gradient of Objective function ----------------------- #\n",
    "def g (party_spending):\n",
    "    # ----- Initialize variables to be used in evaluation ----- #\n",
    "    # Set keras model filename location\n",
    "    #keras_model_filename = 'Keras-Models/bin_model.h5'   # binary classification model\n",
    "    #keras_model_filename = 'Keras-Models/dem_model_short_new.h5'\n",
    "    keras_model_filename = 'Keras-Models/nnet_model.h5'\n",
    "    # Set X_scens to contain the input data for all problem scenarios\n",
    "    X_scens = np.load('Numpy-Array-Files/Input_scenarios.npy')\n",
    "    # Set p_scens to contain the probability data for all problem scenarios\n",
    "    P_scens = np.load('Numpy-Array-Files/Probability_scenarios.npy')\n",
    "    # Set n_scens to be the number of scenarios\n",
    "    n_scens = len(P_scens)\n",
    "    # Set n_districts to be the number of districts\n",
    "    n_districts = X_scens.shape[1]\n",
    "    # Set n_features to be the number of features\n",
    "    n_features = X_scens.shape[2]\n",
    "    # Array to store values from evaluating keras_model\n",
    "    district_outputs = np.array((n_scens, n_districts))\n",
    "    \n",
    "    # ----- Import new party spending values into scenario data ----- #\n",
    "    # Update X_scens to reflect any changes in party spending (in this case changing amount of democrat spending)\n",
    "    for i in range(n_scens):\n",
    "        for j in range(n_districts):\n",
    "            # Assign (n_districts*i + j)th element of party_spending to X_scens[i,j,1] ('1' is 'Democrat Spending' index)\n",
    "            X_scens[i,j,1] = party_spending[n_districts * i + j]\n",
    "    \n",
    "    # ----- Set up keras model to evaluate gradient ----- #\n",
    "    model = load_model(keras_model_filename)\n",
    "    \n",
    "    # Initialize output tensor for neural network model\n",
    "    OutTensor = model.output\n",
    "\n",
    "    # List of input variables for Tensor\n",
    "    InVars = model.inputs\n",
    "    #print(InVars)\n",
    "\n",
    "    # Compute Gradients of model with respect to input variables\n",
    "    Gradients = backend.gradients(OutTensor, InVars)\n",
    "    #print(Gradients)\n",
    "\n",
    "    # Evaluate Gradients\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "    # ----- Compute objective value as expected value of probability democrat candidate will win ----- #\n",
    "    # Initialize grad to zero vector of length n_scenarios * n_districts\n",
    "    grad = np.zeros(n_scens * n_districts)\n",
    "    for i in range(n_scens):\n",
    "        for j in range(n_districts):\n",
    "            # Evaluate gradients at values for scenario i and district j\n",
    "            eval_gradients = sess.run(Gradients, feed_dict={model.input: X_scens[i][j].reshape(1,n_features)})\n",
    "            # Extract derivatives of objective with respect to Democratic and Republican Spending\n",
    "            #repub_spending_derivative = eval_gradients[0][0][0]\n",
    "            #dem_spending_derivative = eval_gradients[0][0][1]\n",
    "            # Set (n_districts * i + j)th component of gradient value from tensorflow\n",
    "            grad[n_districts*i + j] = - P_scens[i] * eval_gradients[0][0][1]\n",
    "            \n",
    "    # Close tensorflow session\n",
    "    sess.close()\n",
    "            \n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------- Batch Objective function (more efficient computation) ------------------------------- #\n",
    "def f_batch (party_spending, model, district_number):\n",
    "    # ----- Initialize variables to be used in evaluation ----- #\n",
    "    # Set keras model filename location\n",
    "    #keras_model_filename = 'Keras-Models/bin_model.h5'   # binary classification model\n",
    "    #keras_model_filename = 'Keras-Models/dem_model_short_new.h5'\n",
    "    keras_model_filename = 'Keras-Models/nnet_model.h5'\n",
    "    # Set filnames needed to load X and p data\n",
    "    Xname = 'Numpy-Array-Files/Input-scenarios-Districts-' + str(district_number) + '.npy'\n",
    "    pname = 'Numpy-Array-Files/Probability-scenarios-Districts-' + str(district_number) + '.npy'\n",
    "    # Set X_scens to contain the input data for all problem scenarios\n",
    "    X_scens = np.load(Xname)\n",
    "    # Set p_scens to contain the probability data for all problem scenarios\n",
    "    P_scens = np.load(pname)\n",
    "    # Set n_scens to be the number of scenarios\n",
    "    n_scens = len(P_scens)\n",
    "    # Set n_districts to be the number of districts\n",
    "    n_districts = X_scens.shape[1]\n",
    "    # Array to store values from evaluating keras_model\n",
    "    district_outputs = np.array((n_scens, n_districts))\n",
    "    \n",
    "    # ----- Import new party spending values into scenario data ----- #\n",
    "    # Update X_scens to reflect any changes in party spending (in this case changing amount of democrat spending)\n",
    "    for i in range(n_scens):\n",
    "        for j in range(n_districts):\n",
    "            # Assign (n_districts*i + j)th element of party_spending to X_scens[i,j,1] ('1' is 'Democrat Spending' index)\n",
    "            X_scens[i,j,1] = party_spending[n_districts * i + j]\n",
    "    \n",
    "    # ----- Set up keras model to evaluate function ----- #\n",
    "    #model = load_model(keras_model_filename)\n",
    "            \n",
    "    # ----- Compute objective value as expected value of probability democrat candidate will win ----- #\n",
    "    # Initialize value to 0\n",
    "    value = 0\n",
    "    for i in range(n_scens):\n",
    "        # Initialize temp to 0\n",
    "        temp = 0\n",
    "        # Evaluate keras model for current scenario data\n",
    "        arr_temp = model.predict(X_scens[i])\n",
    "        #print(\"arr_temp = \", arr_temp)\n",
    "\n",
    "        # Sum all district values up in ith scenario\n",
    "        for j in range(n_districts):\n",
    "            # Add district_output value for jth district to temp\n",
    "            #temp = temp + arr_temp[j][1]      # this line is using binary classification model\n",
    "            temp = temp + arr_temp[j]        # this line if predicting percentage of votes\n",
    "\n",
    "        # Multiply temp by probability scenario i occurs then add to value\n",
    "        value = value + P_scens[i] * temp\n",
    "\n",
    "    return -value\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- Batch Gradient of Objective function ----------------------- #\n",
    "def g_batch (party_spending, Gradients, sess, batch):\n",
    "    # ----- Initialize variables to be used in evaluation ----- #\n",
    "    # Set keras model filename location\n",
    "    #keras_model_filename = 'Keras-Models/bin_model.h5'   # binary classification model\n",
    "    #keras_model_filename = 'Keras-Models/dem_model_short_new.h5'\n",
    "    keras_model_filename = 'Keras-Models/nnet_model.h5'\n",
    "    # Set filnames needed to load X and p data\n",
    "    Xname = 'Numpy-Array-Files/Input-scenarios-Districts-' + str(district_number) + '.npy'\n",
    "    pname = 'Numpy-Array-Files/Probability-scenarios-Districts-' + str(district_number) + '.npy'\n",
    "    # Set X_scens to contain the input data for all problem scenarios\n",
    "    X_scens = np.load(Xname)\n",
    "    # Set p_scens to contain the probability data for all problem scenarios\n",
    "    P_scens = np.load(pname)\n",
    "    # Set n_scens to be the number of scenarios\n",
    "    n_scens = len(P_scens)\n",
    "    # Set n_districts to be the number of districts\n",
    "    n_districts = X_scens.shape[1]\n",
    "    # Set n_features to be the number of features\n",
    "    n_features = X_scens.shape[2]\n",
    "    # Array to store values from evaluating keras_model\n",
    "    district_outputs = np.array((n_scens, n_districts))\n",
    "    \n",
    "    # ----- Import new party spending values into scenario data ----- #\n",
    "    # Update X_scens to reflect any changes in party spending (in this case changing amount of democrat spending)\n",
    "    for i in range(n_scens):\n",
    "        for j in range(n_districts):\n",
    "            # Assign (n_districts*i + j)th element of party_spending to X_scens[i,j,1] ('1' is 'Democrat Spending' index)\n",
    "            X_scens[i,j,1] = party_spending[n_districts * i + j]\n",
    "            \n",
    "    # ----- Compute objective value as expected value of probability democrat candidate will win ----- #\n",
    "    # Initialize grad to zero vector of length n_scenarios * n_districts\n",
    "    grad = np.zeros(n_scens * n_districts)\n",
    "    for i in batch:\n",
    "        j = i % 3               # District number\n",
    "        k = int((i - j)/n_districts) # Scenario number\n",
    "        # Evaluate gradients at values for scenario i and district j\n",
    "        eval_gradients = sess.run(Gradients, feed_dict={model.input: X_scens[k][j].reshape(1,n_features)})\n",
    "        # Extract derivatives of objective with respect to Democratic and Republican Spending\n",
    "        #repub_spending_derivative = eval_gradients[0][0][0]\n",
    "        #dem_spending_derivative = eval_gradients[0][0][1]\n",
    "        # Set (n_districts * i + j)th component of gradient value from tensorflow\n",
    "        grad[i] = - P_scens[k] * eval_gradients[0][0][1]\n",
    "            \n",
    "    return grad\n",
    "\n",
    "\n",
    "# ----------- Stochastic Gradient Descent for Objective function ---------- #\n",
    "def stoch_grad_proj(f_batch, g_batch, Gradients, sess, x, bounds, batch_size, n_scens, line_search, model, dist_number):\n",
    "    # Generate indices to keep for gradient descent\n",
    "    batch = np.random.choice(n_scens, batch_size, replace = False)\n",
    "    #print(\"Current batch: \", batch)\n",
    "    \n",
    "    # ------------ Compute gradient values --------- #    \n",
    "    # Initialize gradient values\n",
    "    #start = time.time()\n",
    "    gradient = np.copy(g_batch(x, Gradients, sess, batch))\n",
    "    #end = time.time()\n",
    "    #print(\"Gradient Time elapsed: \", end - start)\n",
    "    # Set components of gradient not in current batch equal to zero\n",
    "    for i in range(n_scens):\n",
    "        if i not in batch:\n",
    "            gradient[i] = 0\n",
    "    \n",
    "    # ------------ Line Search (if requested) --------- #\n",
    "    # Initialize alpha parameter for line search\n",
    "    alpha = 1\n",
    "    # Check if line search was requested\n",
    "    if line_search == True: \n",
    "        # Initialize parameter for line search\n",
    "        c = 0.5\n",
    "        # Perform armijo line search\n",
    "        while f_batch(x, model, dist_number) - f_batch(x - alpha * gradient, model, dist_number) < - alpha * c * np.dot(gradient, gradient):\n",
    "            # Reduce alpha size and try again\n",
    "            alpha = 0.5 * alpha\n",
    "    \n",
    "    # Update x value\n",
    "    xnew = x - alpha * gradient\n",
    "    \n",
    "    # ------------ Gradient Projection --------- #\n",
    "    # Project values of x back onto bounds if necessary\n",
    "    for i in batch:\n",
    "        # Check if xnew is less than lower bound\n",
    "        if xnew[i] < bounds[i][0]:\n",
    "            #print(\"below lower\")\n",
    "            # Set xnew to lower bound\n",
    "            xnew[i] = bounds[i][0]\n",
    "        # Check if xnew exceeds upper bound\n",
    "        elif xnew[i] > bounds[i][1]:\n",
    "            #print(\"exceeds upper\")\n",
    "            # Set xnew to upper bound\n",
    "            xnew[i] = bounds[i][1]\n",
    "            \n",
    "    # ---------- Return new x value ------------ #\n",
    "    return xnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every county in our state, generate uniformly distributed points in county representing 0.1 percent of population in county\n",
    "# Set number of districts\n",
    "num_districts = 3\n",
    "# Set number of counties\n",
    "num_counties = 37\n",
    "# Set number of races\n",
    "num_races = 2\n",
    "\n",
    "# Generate uniformly distributed population data\n",
    "county_pop = df_florida[['Total; Estimate; Total population']].values\n",
    "county_pop = county_pop.flatten()\n",
    "#county_pop = np.random.permutation(county_pop)\n",
    "#print(county_pop)\n",
    "grid_scale = 100000 # x and y dimensions of each county\n",
    "pop_scale = 1000  # number of people each point on the plot corresponds to\n",
    "\n",
    "\n",
    "# Determine total population\n",
    "total_pop = int(sum(county_pop))\n",
    "\n",
    "# Initialize arrays for x, y coordinates and race percentages of counties\n",
    "x = []\n",
    "y = []\n",
    "county_array = []\n",
    "R = [[] for i in range(num_races)]\n",
    "\n",
    "# --------------- Extract race percentages for each county ------------------ #\n",
    "race_list = ['Total; Estimate; RACE AND HISPANIC OR LATINO ORIGIN - One race - White',\n",
    "             'Total; Estimate; RACE AND HISPANIC OR LATINO ORIGIN - One race - Black or African American']\n",
    "\n",
    "# Initialize data frames containing race data\n",
    "df_races = df_florida[race_list].copy()\n",
    "\n",
    "# Convert each value from raw value to decimal (representing percentage)\n",
    "for mystr in race_list:\n",
    "    # Convert percentage to value\n",
    "    df_races[mystr] = df_races[mystr] / df_florida['Total; Estimate; Total population']\n",
    "\n",
    "# Create array containing race distributions\n",
    "R_dist = df_races.values.T\n",
    "# To use randomized race data, use the following command to generate R_dist\n",
    "#R_dist = np.random.dirichlet(5*np.ones(num_races), num_counties).T\n",
    "\n",
    "# Initialize dimensions for state\n",
    "dimen = math.ceil(math.sqrt(num_counties))\n",
    "\n",
    "# Populate the arrays x, y, and R using randomly generated data\n",
    "county_num = 0\n",
    "\n",
    "for i in range(dimen):\n",
    "    # Initialize population distribution data\n",
    "    for j in range(dimen):\n",
    "        num_points = int(county_pop[county_num]/pop_scale)\n",
    "        t1 = np.random.uniform(grid_scale*i,grid_scale*(i+1),num_points)\n",
    "        t2 = np.random.uniform(grid_scale*j,grid_scale*(j+1),num_points)\n",
    "        x = np.append(x, t1)\n",
    "        y = np.append(y, t2)\n",
    "        county_array = np.append(county_array, county_num*np.ones(num_points))\n",
    "        # Initialize race distribution data\n",
    "        for k in range(num_races):\n",
    "            temp = np.ones(num_points)*R_dist[k][j]\n",
    "            R[k] = np.append(R[k], temp)\n",
    "        # Update counties done counter\n",
    "        county_num = county_num + 1\n",
    "        # Check if all counties have been populated\n",
    "        if county_num >= num_counties:\n",
    "            break;\n",
    "    # Check if all counties have been populated\n",
    "    if county_num >= num_counties:\n",
    "        break;\n",
    "            \n",
    "# Convert each numpy array in R to a list\n",
    "for k in range(num_races):\n",
    "    R[k] = R[k].tolist()\n",
    "            \n",
    "# Generate mean of race percentages in state\n",
    "mu_R = []\n",
    "for i in range(num_races):\n",
    "    mu_R = np.append(mu_R, np.mean(R[i])) \n",
    "    \n",
    "# Combine x and y into vector of ordered pairs\n",
    "P = np.column_stack((x,y))\n",
    "\n",
    "# Determine population per district\n",
    "points_per_dist = int(len(P)/num_districts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following point to generate random starting points for districts\n",
    "#pt = np.random.uniform(0,600000,(num_districts,2))\n",
    "#print(pt)\n",
    "\n",
    "# Set of points for generating districts\n",
    "pts = np.array([[[428596.73182419, 565472.09203069], [406481.47389631,  76214.63431997], [529037.777425,   301312.41034136]],\n",
    "                [[103815.07274117, 523223.20943294], [127784.24278143, 367946.5601808 ], [405265.50666261, 126535.48730887]],\n",
    "                [[232498.50976297, 333876.44175454], [434856.49421111, 536007.79553189], [304201.82051287,  51653.77351857]],\n",
    "                [[264530.56995353, 453923.23875946], [439665.38813067, 440157.66435593], [129802.05652538, 421065.93395801]],\n",
    "                [[577596.93197487, 221554.65871974], [393323.71962208, 576595.73225033], [356491.77699861, 193253.70606583]],\n",
    "                [[ 10069.19911696,  27137.21980555], [541840.3264807,    4792.29529087], [134325.7114619,  582525.18892094]],\n",
    "                [[345564.19987199, 529750.04305769], [354862.82145347, 245082.66827711], [473278.58737241,   7581.06023634]],\n",
    "                [[123099.25101014,   6655.89064585], [ 18575.57679205, 245189.24132337], [321515.0611377,  530000.62871451]]])\n",
    "\n",
    "#print(pts)\n",
    "\n",
    "# Number or nearest neighbors to use in initial construction of districts\n",
    "number_of_neighbors = 4\n",
    "\n",
    "# Set point name\n",
    "district_number = 0\n",
    "i = district_number\n",
    "\n",
    "# Loop over all initial district points and generate local minimums\n",
    "for i in range(8):\n",
    "    # Generate district and return x, y coordinates, indices of points in P and mean of races in each district\n",
    "    xdk, ydk, indk, r_mean = construct_district(num_districts, number_of_neighbors, pts[i], P, R, 1)\n",
    "\n",
    "    # Generate nearest neighbors model\n",
    "    nn_model = initial_district_classification(P, indk, i)\n",
    "\n",
    "    # Generate district from nearest neighbor classification\n",
    "    Y_dist = final_district_classification(P, nn_model, i)\n",
    "\n",
    "    # Using district information, compile County data into District data\n",
    "    n_scenarios, budget_max = county_to_district(df_florida, df_florida_header_list, num_districts, num_counties, county_array, Y_dist, i)\n",
    "\n",
    "    # ----- Compute local minimum for district ----- #\n",
    "    # Set number of decision variables\n",
    "    n_dec_vars = n_scenarios * num_districts\n",
    "\n",
    "    # Define bound constraints on the variables\n",
    "    budget_bounds = np.stack((np.zeros(n_dec_vars), budget_max), axis = -1)\n",
    "\n",
    "    # ----- Set up keras model to evaluate gradient ----- #\n",
    "    # Initialize tensorflow session first\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Initialize keras model from training\n",
    "    keras_model_filename = 'Keras-Models/nnet_model.h5'\n",
    "    model = load_model(keras_model_filename)\n",
    "\n",
    "    # Initialize output tensor for neural network model\n",
    "    OutTensor = model.output\n",
    "\n",
    "    # List of input variables for Tensor\n",
    "    InVars = model.inputs\n",
    "    #print(InVars)\n",
    "\n",
    "    # Compute Gradients of model with respect to input variables\n",
    "    Gradients = backend.gradients(OutTensor, InVars)\n",
    "    #print(Gradients)\n",
    "\n",
    "    # Set filename for writing data\n",
    "    file_name = 'Finalized-Districts/Local-Minimizer/Districts-' + str(i) + '-local-minimizer.txt'\n",
    "    file = open(file_name, 'w')\n",
    "\n",
    "    # Find three different local minimums\n",
    "    for j in range(3):\n",
    "        # Set initial guess\n",
    "        print(\"District %i - Point %i\" %(i,j))\n",
    "        #x0 = np.random.uniform(0, 1, X_scenarios.shape[0] * X_scenarios.shape[1])\n",
    "        x0 = (3-j) * np.ones(n_dec_vars) / 3\n",
    "        # Run stochastic gradient projection on objective function\n",
    "        #start = time.time()\n",
    "        for k in range(2001):\n",
    "            # Most of the time run the stochastic gradient projection with line search\n",
    "            if k % 100 != 0:\n",
    "                x0 = stoch_grad_proj(f_batch, g_batch, Gradients, sess, x0, budget_bounds, 5, n_dec_vars, True, model, i)\n",
    "            else:\n",
    "                x0 = stoch_grad_proj(f_batch, g_batch, Gradients, sess, x0, budget_bounds, 5, n_dec_vars, False, model, i)\n",
    "            # Print out values every 400th iteration\n",
    "            if k % 400 == 0:\n",
    "                # End timer\n",
    "                #end = time.time()\n",
    "                # Print current statistics\n",
    "                #print(\"Time elapsed: \", end - start)\n",
    "                #print(\"x_\", k, \":\", x0)\n",
    "                print(\"f(x_\", k, \") =\", f_batch(x0, model, i))\n",
    "                # Reset timer\n",
    "                #start = time.time()\n",
    "\n",
    "        # Make filename to save solution point\n",
    "        local_min_name = 'Numpy-Array-Files/' + str(i) + 'local_minimizer' + str(j) + '.npy'\n",
    "\n",
    "        # Save final iterate\n",
    "        np.save(local_min_name, x0)\n",
    "\n",
    "        # Convert values in final iterate to dollar amounts by multiplying by 4000000\n",
    "        x_dollars = 4000000 * x0\n",
    "\n",
    "        # Print statistics for state population\n",
    "        file.write(\"-------- District %i - Solution %i --------\\n\" %(i,j))\n",
    "        file.write(\"x    = \")\n",
    "        for l in range(len(x_dollars)):\n",
    "            file.write(str(round(x_dollars[l], 2)))\n",
    "            file.write(\", \")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"f(x) = \")\n",
    "        file.write(str(f_batch(x0, model, i)[0]))\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "    # Close file\n",
    "    file.close()\n",
    "\n",
    "    # Close tensorflow session\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "# Print means of races in districts and actual mean of races from population data\n",
    "#print(r_mean)\n",
    "#print(mu_R) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm_model = initial_district_classification(P, indk, i)\n",
    "\n",
    "# Points that have worked well so far in generating contiguous districts\n",
    "#pt = [[428596.73182419 565472.09203069], [406481.47389631  76214.63431997], [529037.777425   301312.41034136]]\n",
    "#pt = [[103815.07274117 523223.20943294], [127784.24278143 367946.5601808 ], [405265.50666261 126535.48730887]]\n",
    "#pt = [[232498.50976297 333876.44175454], [434856.49421111 536007.79553189], [304201.82051287  51653.77351857]]\n",
    "#pt = [[264530.56995353 453923.23875946], [439665.38813067 440157.66435593], [129802.05652538 421065.93395801]]\n",
    "#pt = [[577596.93197487 221554.65871974], [393323.71962208 576595.73225033], [356491.77699861 193253.70606583]]\n",
    "#pt = [[ 10069.19911696  27137.21980555], [541840.3264807    4792.29529087], [134325.7114619  582525.18892094]]\n",
    "#pt = [[345564.19987199 529750.04305769], [354862.82145347 245082.66827711], [473278.58737241   7581.06023634]]\n",
    "#pt = [[123099.25101014   6655.89064585], [ 18575.57679205 245189.24132337], [321515.0611377  530000.62871451]]\n",
    "\n",
    "\n",
    "# ------ Found when using randomized race data (not sure if good for actual data)\n",
    "#pt = [[380002.71426247 342680.65629285], [367484.54563801 156618.81390881], [420636.80961064 508608.51930695]]\n",
    "#pt = [[350738.54781446 432803.9468541 ], [ 14892.65777381 133318.63679456], [225162.24348173 188512.12569184]]\n",
    "#pt = [[ 42186.45717472 583606.98205745], [492200.10479782 312295.99733307], [295188.97835625 272046.10439235]]\n",
    "\n",
    "# Extra code leftover from testing\n",
    "# ----------------- Generate sample plot of initial data points ----------------- #\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "#ax2 = fig.add_subplot(111, aspect='equal')\n",
    "#for p in [\n",
    "#    patches.Rectangle((0, 0), 1000, 1000, fill=False),\n",
    "#    patches.Rectangle((0, 1000), 1000, 2000, fill=False),\n",
    "#]:\n",
    "#    ax2.add_patch(p)\n",
    "plt.plot(x,y,'bo')\n",
    "plt.show()\n",
    "\n",
    "# ----------------- Testing objective function and gradients -------------------- #\n",
    "party_spend = np.random.uniform(0, 1, X_scenarios.shape[0] * X_scenarios.shape[1])\n",
    "print(party_spend)\n",
    "\n",
    "sdf = f(party_spend)\n",
    "\n",
    "print(\"objective value = \", sdf)\n",
    "\n",
    "sdf = g(party_spend)\n",
    "\n",
    "print(sdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
